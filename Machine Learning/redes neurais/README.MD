# Redes neurais

- [1. Introdu√ß√£o](#1-introdu√ß√£o)
- [2. Estrutura](#2-estrutura)
- [3. Como as redes neurais aprendem](#3-como-as-redes-neurais-aprendem)
- [4. Tipos](#4-tipos)
- [5. Vantagens e desvantagens](#3-vantagens-e-desvantagens)

## 1. Introdu√ß√£o

> **DEFINI√á√ÉO:** √© um modelo inspirado no funcionamento do c√©rebro humano.

Elas s√£o compostas por unidades chamadas neur√¥nios ou n√≥s, que est√£o organizados em camadas. Cada neur√¥nio recebe um conjunto de entradas, realiza um c√°lculo e gera uma sa√≠da.

Podem ser aplicadas em problemas complexos, como reconhecimento de imagem, processamento de linguagem natural, e jogos.

## 2. Estrutura

- Cada neur√¥nimo √© chamado de preceptron
- As **camadas de entrada (input layers)** recebem os dados brutos (entradas) que a rede ir√° processar. Cada neur√¥nio nesta camada corresponde a uma caracter√≠stica ou atributo do dado.
- As **camadas ocultas (hidden layers)** s√£o as camadas entre a entrada e a sa√≠da. Aqui, os neur√¥nios processam as entradas e transmitem informa√ß√µes para a pr√≥xima camada. O n√∫mero de camadas e neur√¥nios em cada camada pode variar, o que influencia a complexidade da rede.
- A **camada de sa√≠da (output layer)** gera a previs√£o ou o resultado da rede. Dependendo do tipo de problema, o n√∫mero de neur√¥nios nesta camada varia. Por exemplo, para um problema de classifica√ß√£o bin√°ria, haver√° um √∫nico neur√¥nio de sa√≠da, enquanto para uma classifica√ß√£o multiclasse, pode haver um neur√¥nio para cada classe.

### 2.1 Fun√ß√£o de ativa√ß√£o

> **DEFINI√á√ÉO:** componente que determina se um neur√¥nio deve ser ativado ou n√£o, ou seja, se a informa√ß√£o recebida por esse neur√¥nio ser√° transmitida para a pr√≥xima camada.

la introduz n√£o linearidade no modelo, permitindo que redes neurais aprendam e representem padr√µes complexos, como os encontrados em imagens, texto ou s√©ries temporais.

Sem fun√ß√µes de ativa√ß√£o, as redes neurais seriam apenas combina√ß√µes lineares de entradas, incapazes de resolver problemas complexos. Por isso, a fun√ß√£o de ativa√ß√£o √© crucial para que as redes neurais possam aprender tarefas como reconhecimento de imagem, tradu√ß√£o de linguagem ou an√°lise de sentimentos.

#### 2.1.1 Tipos
1. **Sigmoid** (fun√ß√£o log√≠stica)
    - F√≥rmula: 
    ```math
        \sigma(x) = \frac{1}{1 + e^{-x}}
    ```
    - Sa√≠da: Entre 0 e 1.
    - Caracter√≠sticas:
        - A fun√ß√£o comprime a sa√≠da para um intervalo de 0 a 1, o que pode ser interpretado como uma probabilidade.
    - **Desvantagem**: O problema do desvanecimento do gradiente (vanishing gradient), onde a derivada se aproxima de 0 para valores muito altos ou muito baixos de entrada, o que pode dificultar o treinamento de redes mais profundas.
    - **Aplica√ß√£o**: √â utlizadoe m tarefas onde a sa√≠da precisa ser uma probabilidade ou em problemas de classifica√ß√£o bin√°ria.

2. **Tanh** (tangente hiperb√≥lica)
    - F√≥rmula: 
    ```math
        \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
    ```
    - Sa√≠da: Entre -1 e 1.
    - Caracter√≠sticas:
        - A tangente hiperb√≥lica √© semelhante √† sigmoid, mas a sa√≠da √© centrada em torno de zero, o que pode ajudar a melhorar o desempenho durante o treinamento.
        - Pode ser √∫til em redes neurais com entradas que podem assumir valores positivos e negativos.
    - **Desvantagem**: Assim como a sigmoid, a tanh tamb√©m sofre de desvanecimento do gradiente, o que pode dificultar o treinamento de redes muito profundas.

3. **ReLU** (rectified linear unit)
    - F√≥rmula: 
    ```math
        \text{ReLU}(x) = \max(0, x)
    ```
    - Sa√≠da: Pode ser qualquer valor real, dependendo do par√¢metro ùõº
    - Caracter√≠sticas:
        - Tenta resolver o problema do neur√¥nio morto permitindo que, para entradas negativas, a fun√ß√£o tenha uma pequena inclina√ß√£o.

    - **Desvantagem**: O valor de ùõº precisa ser ajustado corretamente, o que pode tornar o treinamento um pouco mais complexo.
    
4. **Linear**
    - F√≥rmula:
     ```math
        f(x) = x
    ```
    - Sa√≠da: 
    - Caracter√≠sticas:
        - Todas as fun√ß√µes de ativa√ß√£o entre as camadas s√£o lineares, ou seja, cada neur√¥nio realiza uma simples multiplica√ß√£o e soma dos inputs.

    - **Desvantagem**: N√£o consegue aprender padr√µes complexos, pois, apesar de ter v√°rias camadas, as sa√≠das de cada camada ainda podem ser representadas como uma combina√ß√£o linear das entradas. O que significa que, independentemente de quantas camadas voc√™ tenha, a rede ser√° equivalente a uma √∫nica camada linear. Portanto, ela tem a mesma capacidade que uma regress√£o linear.
    - **Aplica√ß√£o**: utilizadas principalmente em problemas de regress√£o, onde o objetivo √© prever uma sa√≠da cont√≠nua com base em entradas lineares.
    

## 3. Como as redes neurais aprendem

O aprendizado em uma rede neural √© realizado por meio de um processo chamado retropropaga√ß√£o (backpropagation). Esse processo envolve os seguintes passos:

1. **Forward pass (passagem direta)**: A entrada √© passada pela rede, camada por camada, at√© a camada de sa√≠da, onde a previs√£o √© gerada.

2. **C√°lculo do erro**: O erro √© calculado comparando a previs√£o gerada pela rede com o valor real (r√≥tulo). O erro pode ser medido usando uma fun√ß√£o de perda (por exemplo, erro quadr√°tico m√©dio ou entropia cruzada).

3. **Backward pass (retropropaga√ß√£o)**: A rede ajusta seus pesos e vieses para reduzir o erro. Isso √© feito usando o algoritmo de gradiente descendente, que calcula as derivadas da fun√ß√£o de perda em rela√ß√£o aos pesos e faz ajustes nos pesos para minimizar o erro.

4. **Itera√ß√£o**: Esse processo √© repetido muitas vezes (em v√°rias √©pocas), com a rede aprendendo aos poucos como fazer previs√µes mais precisas.

## 4. Tipos

### 4.1 Perceptron (MLP - Multi-Layer Perceptron)

O tipo mais simples de rede neural, com uma ou mais camadas ocultas, usado para tarefas como classifica√ß√£o e regress√£o.

### 4.2 Redes Neurais Convolucionais (CNN - Convolutional Neural Networks)

Usadas principalmente em processamento de imagens e v√≠deo. Elas aplicam convolu√ß√µes para extrair caracter√≠sticas locais de uma imagem (como bordas, texturas) e s√£o muito boas para reconhecimento visual.

### 4.3 Redes Neurais Recorrentes (RNN - Recurrent Neural Networks)

Projetadas para lidar com dados sequenciais, como s√©ries temporais ou texto. Elas t√™m conex√µes "recurentes" que permitem que informa√ß√µes anteriores influenciem o processamento atual.

### 4.4 Redes Generativas (GANs - Generative Adversarial Networks):

Compostas por duas redes neurais que competem entre si: uma geradora, que cria amostras de dados, e uma discriminadora, que tenta distinguir entre dados reais e gerados. GANs s√£o usadas em tarefas como gera√ß√£o de imagens realistas e deepfakes.

## 5. Vantagens e desvantagens

Vantagens das redes neurais:
Capacidade de aprender representa√ß√µes complexas: Elas s√£o √≥timas em aprender padr√µes em grandes volumes de dados, principalmente quando esses dados s√£o altamente n√£o lineares ou complexos.
Versatilidade: Podem ser aplicadas a uma grande variedade de problemas, desde imagem at√© texto, √°udio, e muito mais.
Adaptabilidade: Elas podem melhorar com o tempo √† medida que recebem mais dados e podem aprender caracter√≠sticas que n√£o s√£o explicitamente programadas.
Desvantagens:
Necessidade de grandes quantidades de dados: Para obter bons resultados, redes neurais geralmente precisam de grandes volumes de dados para treinamento.
Consumo computacional: O treinamento de redes neurais, especialmente redes profundas, pode ser muito intensivo em termos de tempo e recursos computacionais.
Dificuldade de interpreta√ß√£o: As redes neurais s√£o frequentemente chamadas de "caixas-pretas", j√° que seus processos internos podem ser dif√≠ceis de interpretar e entender.